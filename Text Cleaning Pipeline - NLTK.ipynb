{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize as wt\n",
    "from nltk import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUse the NLTK package to try to better identify the industries and wrongful acts present in the majority of the text.  \\nThen use this information as the basis for the dictionary that you will create to categorize each text by industry and \\nwrongful act. \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Objective\n",
    "'''\n",
    "Use the NLTK package to try to better identify the industries and wrongful acts present in the majority of the text.  \n",
    "Then use this information as the basis for the dictionary that you will create to categorize each text by industry and \n",
    "wrongful act. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Concatenaed Text\n",
    "\n",
    "chdir = os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Corpus Texts/')\n",
    "File = open('EEOC Articles - Concatenated.txt')\n",
    "Text = File.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text\n",
    "Text_tokenized = word_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Cleaning Packages\n",
    "chdir = os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Modules')\n",
    "\n",
    "def create_dict_punct():\n",
    "    import string\n",
    "    Dict = {}\n",
    "    Punct = string.punctuation\n",
    "    for x in Punct:\n",
    "        Dict[x] = ''\n",
    "    return Dict \n",
    "\n",
    "def create_dict_stopwords():\n",
    "    from nltk.corpus import stopwords\n",
    "    Stopwords = stopwords.words('english')                  \n",
    "    Dict = {}\n",
    "    for x in Stopwords:\n",
    "        Dict[x] = ''\n",
    "    return Dict\n",
    "\n",
    "def strip_punctuation(Token_list):\n",
    "    dict_punct = create_dict_punct()\n",
    "    List = []\n",
    "    for x in Token_list:\n",
    "        if x not in dict_punct:\n",
    "            List.append(x)\n",
    "    return List\n",
    "    \n",
    "def strip_stop_words(Token_list):\n",
    "    stop_words = create_dict_stopwords()\n",
    "    List = []\n",
    "    for x in Token_list:\n",
    "        if x not in stop_words:\n",
    "            List.append(x)\n",
    "    return List\n",
    "\n",
    "def strip_tokens_forwardSlash_x2(Token_list):\n",
    "    List = []\n",
    "    for token in Token_list:\n",
    "        if '\\\\' not in token and '/' not in token:\n",
    "            \n",
    "            List.append(token)\n",
    "    return List\n",
    "\n",
    "def strip_two_variable_tokens(Token_list):\n",
    "    List = [x for x in Token_list if len(x) > 2]\n",
    "    return List\n",
    "\n",
    "def get_isalpha(Token_list):\n",
    "    List = [x for x in Token_list if x.isalpha()]\n",
    "    return List\n",
    "\n",
    "def get_unusual_english_words(Token_list):\n",
    "    list_vocab = set(w.lower() for w in Token_list if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = list_vocab.difference(english_vocab)  # unclear if 'difference' is being call on a standard list obj\n",
    "    return sorted(unusual)\n",
    "\n",
    "def get_spanish_words_in_text(Token_list):\n",
    "    list_vocab = set(w.lower() for w in Token_list if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.cess_esp.words())\n",
    "    list_spanish_words = list_vocab.difference(english_vocab)  # unclear if 'difference' is being call on a standard list obj\n",
    "    return sorted(unusual)\n",
    "\n",
    "def create_spanish_dictionary():\n",
    "    Spanish_word_dict = {}\n",
    "    for x in nltk.corpus.cess_esp.words():\n",
    "        Spanish_word_dict[x] = ''\n",
    "    return Spanish_word_dict\n",
    "\n",
    "def strip_spanish_words(Token_list):\n",
    "    Spanish_word_dict = create_spanish_dictionary()\n",
    "    List = []\n",
    "    for x in Token_list:\n",
    "        if x not in Spanish_word_dict:\n",
    "            List.append(x)\n",
    "    return List\n",
    "\n",
    "def get_synsets(List_tokens):\n",
    "    List = []\n",
    "    for token in List_tokens:\n",
    "        Synset = nltk.corpus.wordnet.synsets(token)\n",
    "        if Synset == []:\n",
    "            List.append(token)\n",
    "        else:\n",
    "            Synset = Synset[0]\n",
    "            List.append(Synset)\n",
    "    return List\n",
    "\n",
    "def word_count_frequency_table(Text):\n",
    "    Dict = {}\n",
    "    \n",
    "    for x in Text:\n",
    "        if x in Dict.keys():\n",
    "            Dict[x] = Dict[x] + 1\n",
    "        else:\n",
    "            Dict[x] = 1\n",
    "    \n",
    "    df = pd.DataFrame(Dict, index = [1])\n",
    "    df_tran = pd.DataFrame.transpose(df)\n",
    "    df_final = df_tran.sort_values(1, ascending = False)\n",
    "    df_rename = df_final.rename(columns = {1:'Word Count'})\n",
    "    return df_rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Text_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-970254bca935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mText_strip_spanish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mClean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_cleaning_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mText_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Text_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Cleaning Pipeline\n",
    "\n",
    "def text_cleaning_pipeline(Text):\n",
    "    # Convert text to lowercase\n",
    "    Text = [x.lower() for x in Text]  \n",
    "    # Strip punctuation, stopwords, words contaning //\n",
    "    Text_strip_punct = strip_punctuation(Text)\n",
    "    Text_strip_stop_words = strip_stop_words(Text_strip_punct)\n",
    "    Text_strip_forwardslash = strip_tokens_forwardSlash_x2(Text_strip_stop_words)\n",
    "    Text_strip_two_variable_tokens = strip_two_variable_tokens(Text_strip_forwardslash)\n",
    "    Text_get_isalpha = get_isalpha(Text_strip_two_variable_tokens)\n",
    "    Text_strip_spanish = strip_spanish_words(Text_get_isalpha)\n",
    "    return Text_strip_spanish\n",
    "    \n",
    "Clean_text = text_cleaning_pipeline(Text_tokenized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words original text 2284787\n",
      "Num words cleaned text 779166\n",
      "\n",
      "Length set of original text 2284787\n",
      "Length set of cleaned text 18391\n"
     ]
    }
   ],
   "source": [
    "print('Num words original text', len(Text_tokenized))\n",
    "print('Num words cleaned text', len(Clean_text))\n",
    "print('')\n",
    "\n",
    "\n",
    "print('Length set of original text', len(Text_tokenized))\n",
    "print('Length set of cleaned text', len(set(Clean_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Synset List\n",
    "\n",
    "Synsets_list = get_synsets(Clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779166\n"
     ]
    }
   ],
   "source": [
    "print(len(Synsets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_synset(Synset_list):\n",
    "    Synset_clean_list = []\n",
    "    \n",
    "    # Regex Exp - Find\n",
    "    Regex = \"[a-z]+\\.[a-z]\\.[0-9]+\"\n",
    "    Re_compile = re.compile(Regex)\n",
    "    \n",
    "    # Regex loop expression to obtain synset\n",
    "    for synset in Synset_list:\n",
    "        \n",
    "        # Search for Regex Exp\n",
    "        synset = str(synset)\n",
    "        Re_search = re.search(Re_compile, synset)\n",
    "        \n",
    "        if Re_search == None:\n",
    "            Synset_clean_list.append(synset)\n",
    "        \n",
    "        elif '(' not in synset:\n",
    "            Synset_clean_list.append(synset)\n",
    "        \n",
    "        else:\n",
    "            Re_group = Re_search.group()\n",
    "            Re_split = Re_group.split('.')\n",
    "            Synset_clean_list.append(Re_split[0])\n",
    "            \n",
    "    # Return Clean Synset \n",
    "    return Synset_clean_list\n",
    "\n",
    "\n",
    "Clean_synset_list = clean_synset(Synsets_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Synset List =>  779166\n"
     ]
    }
   ],
   "source": [
    "print('Length of Synset List => ', len(Clean_synset_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#### Stemming\n",
    "\n",
    "# Base Object for Stemming\n",
    "class nltk.stem.api.StemmerI[source]\n",
    "Bases: object\n",
    "\n",
    "A processing interface for removing morphological affixes from words. This process is known as stemming.\n",
    "stem(token)[source]\n",
    "Strip affixes from the token and return the stem.\n",
    "\n",
    "# Regex Stemming\n",
    "lass nltk.stem.regexp.RegexpStemmer(regexp, min=0)[source]\n",
    "Bases: nltk.stem.api.StemmerI\n",
    "\n",
    "A stemmer that uses regular expressions to identify morphological affixes. \n",
    "Any substrings that match the regular expressions will be removed.\n",
    "\n",
    "\n",
    "# Wordnet\n",
    "\n",
    "class nltk.stem.wordnet.WordNetLemmatizer[source]\n",
    "Bases: object\n",
    "\n",
    "WordNet Lemmatizer\n",
    "Lemmatize using WordNet’s built-in morphy function. \n",
    "Returns the input word unchanged if it cannot be found in WordNet.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Stem Clean Synset List\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def get_stem_of_token(Token_list):\n",
    "    stemmer = PorterStemmer()\n",
    "    Stem_list = []\n",
    "    \n",
    "    for x in Token_list:\n",
    "        stem = stemmer.stem(x)\n",
    "        Stem_list.append(stem)\n",
    "        \n",
    "    return Stem_list\n",
    "\n",
    "Stem_list = get_stem(Clean_synset_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency Table\n",
    "\n",
    "wc_freq_table = word_count_frequency_table(Stem_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chdir = os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Projects/Web Scraping Project - EEOC Articles')\n",
    "\n",
    "\n",
    "def write_to_excel(dataframe, filename):\n",
    "    import pandas as pd\n",
    "    writer = pd.ExcelWriter(filename+'.xlsx')\n",
    "    dataframe.to_excel(writer, sheet_name = 'Data')\n",
    "    writer.save()\n",
    "    \n",
    "write_to_excel(wc_freq_table, 'EEOC_Articles_Synset_freq_table')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
