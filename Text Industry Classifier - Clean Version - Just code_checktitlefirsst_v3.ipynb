{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import wordnet as wn\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Personal Text Analysis Modules\n",
    "\n",
    "os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Modules')\n",
    "Cwd = os.getcwd()\n",
    "\n",
    "import ccirelli2_general_text_analysis_module as cc_txt_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Excel Doc with Industry Keys and Key Words\n",
    "\n",
    "os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Projects/Web Scraping Project - EEOC Articles')\n",
    "Url = r'EEOC Article Study - Industry Values.xlsx'\n",
    "df1 = pd.read_excel(Url)\n",
    "df2 = df1.set_index('Keys') # Set the Major Industry Group as the index. \n",
    "df3 = df2.iloc[0:22, :]     # Limit the scope of the dataframe.  Function is picking up nan values which is causing\n",
    "                            # an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Articles File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chdir = os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Projects/Web Scraping project - EEOC Articles/EEOC Articles/')\n",
    "Cdw = os.getcwd()\n",
    "File_name_list = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Beautiful Soup Object of Each Article Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bs4Obj_of_Title_from_Texts(File_name_list):\n",
    "        bs4_obj_list = []\n",
    "        \n",
    "        # Get File Direction \n",
    "        for file_name in File_name_list:\n",
    "            if 'txt' in file_name:\n",
    "                Dir = Cdw + '/' + file_name  \n",
    "                File = open(Dir, 'rb')\n",
    "                Text = File.read()\n",
    "                bs4obj = BeautifulSoup(Text, 'html.parser')\n",
    "                h1 = bs4obj.find('h1')\n",
    "                if h1 == None:\n",
    "                    bs4_obj_list.append(None)\n",
    "                else:\n",
    "                    bs4_obj_list.append(str(h1))\n",
    "            \n",
    "        return bs4_obj_list\n",
    "    \n",
    "bs4Obj_List_Titles = get_bs4Obj_of_Title_from_Texts(File_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Diction of Punctuation Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_punct_dict(list):\n",
    "    Dict = {}\n",
    "    for x in list:\n",
    "        Dict[x] = ''\n",
    "    return Dict\n",
    "\n",
    "punct_dict = create_punct_dict(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_title_clean_bs4Objs_returnListObjs(bs4Obj_list):\n",
    "        \n",
    "    # Define list to catch tokenized object\n",
    "    bs4Obj_tokenized_list = []\n",
    "    \n",
    "    # Tokenize each object\n",
    "    for bs4Obj in bs4Obj_list:\n",
    "        bs4Obj_string = str(bs4Obj)\n",
    "        bs4Obj_tokenized = nltk.wordpunct_tokenize(bs4Obj_string)\n",
    "        bs4Obj_is_alpha = cc_txt_analysis.get_isalpha(bs4Obj_tokenized)\n",
    "        bs4Obj_tokenized_lower = [x.lower() for x in bs4Obj_is_alpha]\n",
    "        bs4Obj_tokenized_list.append(bs4Obj_tokenized_lower)\n",
    "    \n",
    "    # Return bs4Obj List of tokenized objects\n",
    "    \n",
    "    return bs4Obj_tokenized_list\n",
    "\n",
    "Tokenized_title_list = tokenize_title_clean_bs4Objs_returnListObjs(bs4Obj_List_Titles[:1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Tokenized version of Text Body - Put Into a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_textBody_cleanText_returnListObjs(File_name_list):\n",
    "        Clean_text_list = []\n",
    "        \n",
    "        for file_name in File_name_list:\n",
    "            if 'txt' in file_name:\n",
    "                Dir = Cdw + '/' + file_name  \n",
    "                File = open(Dir, 'rb')\n",
    "                Text = File.read()\n",
    "                Text_str = str(Text)\n",
    "                Clean_text = cc_txt_analysis.text_cleaning_pipeline_Single_Text_pre_tokenize(Text_str)\n",
    "                Clean_text_list.append(Clean_text)\n",
    "        \n",
    "        return Clean_text_list\n",
    "\n",
    "Tokenized_textBody_list = tokenize_textBody_cleanText_returnListObjs(File_name_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Industry Dictionary of Key Word Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Create_Dictionary_Industry_KeyWord_Synonyms():\n",
    "    os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Projects/Web Scraping Project - EEOC Articles')\n",
    "    Url = r'EEOC Article Study - Industry Values.xlsx'\n",
    "    df1 = pd.read_excel(Url)\n",
    "    df2 = df1.set_index('Keys')  \n",
    "    dataframe = df2.iloc[0:22, :]      #should have just recreated this list in Python. \n",
    "                             \n",
    "    Dict_major = {}\n",
    "    \n",
    "    # for each major industry value (column A) in the dataframe\n",
    "    for major in dataframe.index:\n",
    "        \n",
    "        # Define the minor industry as the row values. \n",
    "        df_minor = dataframe.loc[major]\n",
    "        \n",
    "        # Define the industry value lvl 2 and 3 to include in the keys of your dictionary (for descriptive purp)\n",
    "        Industry_lvl_2 = df_minor['Division']\n",
    "        Industry_lvl_3 = df_minor['Column 1']\n",
    "\n",
    "        # Define the list to catch the synsets generated by the below function. \n",
    "        Synset_list = []\n",
    "                \n",
    "        # For the value in the dataframe, the key being the value in column B 'Division'\n",
    "        for value in df_minor[1:]:\n",
    "            # Verify that it is an instance of a string as we have None values in the dataframe. \n",
    "            if isinstance(value, str):\n",
    "                # Convert the value to lowercase. \n",
    "                value_lower = value.lower()\n",
    "                # Generate synsets for this value.\n",
    "                Synset = wn.wordnet.synsets(value_lower)\n",
    "                # Extract the word from the synset object ('word.n.01')\n",
    "                Lemma_names = [x.lemma_names() for x in Synset]\n",
    "                # Lemman_names is a list of lists.  Iterate over each list. \n",
    "                for List in Lemma_names:\n",
    "                    # Get the words in each sub list. \n",
    "                    for word in List:\n",
    "                        # We want to end up with a Set of unique values.  Therefore, check to see \n",
    "                        # if the word is already in our list. \n",
    "                        if word not in Synset_list:\n",
    "                            # If not, then append the word to our list. \n",
    "                            Synset_list.append(word)\n",
    "        \n",
    "        # Create the name of the Major Industry Group that will constitute the Keys of our Dict. \n",
    "        Industry_identifier = (str(major) + ' ' + Industry_lvl_2 + ' - ' + Industry_lvl_3)\n",
    "        # Join the Keys with our matching values. \n",
    "        Dict_major[Industry_identifier] = Synset_list\n",
    "    \n",
    "    # Return our completed Industry Dictionary\n",
    "    return Dict_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Industry_Dictionary = Create_Dictionary_Industry_KeyWord_Synonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Industry Classifier - Compares Text to Industry Dict & Counts Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Text_Word_Industry_Count(Tokenized_title_list, Tokenized_textBody_list, Dict_Industries):\n",
    "\n",
    "    Word_match_count = {}\n",
    "    \n",
    "    for title, text in zip(Tokenized_title_list, Tokenized_textBody_list): \n",
    "                    \n",
    "            for title_token, body_token in zip(title, text):       \n",
    "            \n",
    "                # for each industry category\n",
    "                for industry_category in Dict_Industries:     \n",
    "        \n",
    "                    # if the title token is found within the industry category values\n",
    "                    if title_token in Dict_Industries[industry_category]:\n",
    "                \n",
    "                        # if it is, check to see if the industry category is already in the Word_match_dict \n",
    "                        if industry_category in Word_match_count.keys():\n",
    " \n",
    "                        # If the industry category is already in the dict, check to see if the title_token has already\n",
    "                        # been added to this key. \n",
    "                            if title_token in Word_match_count[industry_category]:\n",
    "                         \n",
    "                                # if the value already exists for this key, then just add another title token \n",
    "                                Word_match_count[industry_category] = Word_match_count[industry_category] + [title_token]\n",
    "                        \n",
    "                            # if the title token is not present, but we have the key, no prob, just add a token.  \n",
    "                            else: \n",
    "                                Word_match_count[industry_category] = [title_token]\n",
    "                    \n",
    "                        # if the industry category is not already in the keys, but we have a match, add key and dict\n",
    "                        else:\n",
    "                            Word_match_count[industry_category] = [title_token]\n",
    "                \n",
    "                    # Now, if the title token is not in the Dictionary, we dont have a match, so lets see if we can \n",
    "                    # match a token in the text body to our Industry Dictionary. \n",
    "                \n",
    "                    else:\n",
    "                        if body_token in Dict_Industries[industry_category]:\n",
    "                            if industry_category in Word_match_count.keys():\n",
    "                                if body_token in Word_match_count[industry_category]:\n",
    "                                    Word_match_count[industry_category] = Word_match_count[industry_category] + [body_token]\n",
    "                                else:\n",
    "                                    Word_match_count[industry_category] = [body_token]\n",
    "                            else:\n",
    "                                Word_match_count[industry_category] = [body_token]\n",
    "                        # if body_token not in Dict_values, then neither our title_tokens nor body_tokens found \n",
    "                        # a match. \n",
    "                        \n",
    "    for x in Word_match_count:\n",
    "        Word_match_count[x] = len(Word_match_count[x])            \n",
    "    \n",
    "    df = pd.DataFrame(Word_match_count, index = [1])\n",
    "    df_tran = pd.DataFrame.transpose(df)\n",
    "    df_sorted = df_tran.sort_values(1, ascending = False)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "### This is an issue.  You dont know if your two lists match in terms of the order of the articles. \n",
    "### You should have embedded within one function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             1\n",
      "Major Group 73 Services - Business Services  1\n"
     ]
    }
   ],
   "source": [
    "Test = Get_Text_Word_Industry_Count(Tokenized_title_list,\n",
    "                                    Tokenized_textBody_list, \n",
    "                                    Industry_Dictionary)\n",
    "\n",
    "\n",
    "print(Test)\n",
    "\n",
    "# You're missing a loop.  You need to loop over the List of tokenized text, then loop over the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Industry Prediction - Take the top row of the dataframe as the highest num matches as our prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Industry_prediction(Obj_from_industry_classifier):\n",
    "    # Return the top row as the Prediction from our Industry Classifier function. \n",
    "    Top_row = Obj_from_industry_classifier.iloc[0]\n",
    "    Industry = Top_row.name\n",
    "    return Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Major Group 73 Services - Business Services'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Industry_prediction(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Careful, your Classifier is iterating over a list.  Maybe you should embed the prediction in the classifier.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_industry_pipeline(Text, Industry_dataframe):\n",
    "    Text_tokenized = word_tokenize(Text)\n",
    "    Clean_text = text_cleaning_pipeline(Text_tokenized)\n",
    "    Industry_Dictionary = Create_Industry_Dictionary(Industry_dataframe)\n",
    "    Industry_classifier = get_industry(Clean_text, Dict_Industries_dict)\n",
    "    Industry_prediction = Industry_prediction(Industry_classifier)\n",
    "    return Industry_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop over individual texts to get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major Group 87 Services - Accounting\n",
      "Major Group 20-39 Manufacturing - Manufacture\n",
      "Major Group 81 Services - Law firm\n",
      "Major Group 87 Services - Accounting\n",
      "Major Group 91-99 Services - Government\n",
      "Major Group 50-51 Wholesale - Wholesale\n",
      "Major Group 91-99 Services - Government\n",
      "Major Group 60-67 Finance - Bank \n",
      "Major Group 81 Services - Law firm\n",
      "Major Group 73 Services - Business Services\n"
     ]
    }
   ],
   "source": [
    "chdir = os.chdir(r'/Users/ccirelli2/Public/Python Programing Docs/Projects/Web Scraping project - EEOC Articles/EEOC Articles/')\n",
    "Url_name_list = os.listdir()\n",
    "\n",
    "for x in Url_name_list[:10]:\n",
    "    if '.txt' in x:\n",
    "        File = open(x, 'rb')\n",
    "        Text_bytes = File.read()\n",
    "        Text_string = str(Text_bytes)\n",
    "        \n",
    "        Text_tokenized = nltk.word_tokenize(Text_string)\n",
    "        Text_strip_punct = cc.strip_punctuation(Text_tokenized)\n",
    "        Text_strip_stop_words = cc.strip_stop_words(Text_strip_punct)\n",
    "        Text_strip_forwardslash = cc.strip_tokens_forwardSlash_x2(Text_strip_stop_words)\n",
    "        Text_strip_two_variable_tokens = cc.strip_two_variable_tokens(Text_strip_forwardslash)\n",
    "        Text_get_isalpha = cc.get_isalpha(Text_strip_two_variable_tokens)\n",
    "        Final_text = [x.lower() for x in Text_get_isalpha]\n",
    "\n",
    "        \n",
    "        Industry_Dictionary = Create_Industry_Dictionary(df3)\n",
    "        Industry_count = Get_Text_Industry_Count(Final_text, Industry_Dictionary)\n",
    "        print(Industry_prediction(Industry_count))\n",
    "        \n",
    "        #### Something is wrong with the Get_Text_Industry_Count Function.  It is returning empty dataframes. \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
